{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import nltk\n",
    "import collections\n",
    "import sklearn as sk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_cleaning(raw_data):\n",
    "    '''\n",
    "    This function takes in a string specifically \"cyberpunk\" and delete it.\n",
    "    '''\n",
    "    useful_words = raw_data.lower().split()\n",
    "    useful_words = [w.replace('cyberpunk', '') for w in useful_words]\n",
    "    return( \" \".join(useful_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../../B_Data_Cleaning/cleaned_real_reviews.csv\")\n",
    "df[\"Review\"] = df[\"Review\"].astype(\"str\")\n",
    "df['Review'] = df['Review'].apply(data_cleaning)\n",
    "df[\"Review\"] = df[\"Review\"].astype(\"str\")\n",
    "\n",
    "# make a set_column to count the number of words in each review\n",
    "df[\"set_column\"] = df[\"Review\"].apply(lambda x: set(x.split()))\n",
    "df\n",
    "\n",
    "# split the data into train and test\n",
    "train, test = sk.model_selection.train_test_split(df, test_size=0.33, random_state=42)\n",
    "\n",
    "# # divide the df_cleaned based on \"Recommended or Not Recommended\" column\n",
    "# temp_recommended = temp.loc[temp[\"Recommended or Not Recommended\"] == True,:].reset_index(drop=True)\n",
    "# temp_not_recommended = temp.loc[temp[\"Recommended or Not Recommended\"] == False,:].reset_index(drop=True)\n",
    "\n",
    "# # divide temp_recommended into train and test using random sample seed\n",
    "# temp_recommended_train = temp_recommended.sample(frac=0.67, random_state=42).reset_index(drop=True)\n",
    "# temp_recommended_test = temp_recommended.drop(temp_recommended_train.index).reset_index(drop=True)\n",
    "\n",
    "# # divide temp_not_recommended into train and test using random sample seed\n",
    "# temp_not_recommended_train = temp_not_recommended.sample(frac=0.33, random_state=42).reset_index(drop=True)\n",
    "# temp_not_recommended_test = temp_not_recommended.drop(temp_not_recommended_train.index).reset_index(drop=True)\n",
    "\n",
    "# # Combine test dataset\n",
    "# temp_test = pd.concat([temp_recommended_test, temp_not_recommended_test], axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividing dataset into recommended and not recommended\n",
    "df_recom = train.loc[train[\"Recommended or Not Recommended\"] == True,:]\n",
    "df_recom = df_recom.reset_index(drop=True)\n",
    "\n",
    "df_not_recom = train.loc[train[\"Recommended or Not Recommended\"] == False,:]\n",
    "df_not_recom = df_not_recom.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is to make the set dictionary\n",
    "# count how many times each word appears in the reviews\n",
    "bow_recom_temp = collections.Counter([y for x in df_recom.set_column for y in x])\n",
    "bow_not_recom_temp = collections.Counter([y for x in df_not_recom.set_column for y in x])\n",
    "\n",
    "bow_recom_set = dict(bow_recom_temp)\n",
    "bow_not_recom_set = dict(bow_not_recom_temp)\n",
    "\n",
    "for key in bow_recom_set:\n",
    "    if key not in bow_not_recom_set:\n",
    "        bow_not_recom_set[key] = 0\n",
    "        pass\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "for key in bow_not_recom_set:\n",
    "    if key not in bow_recom_set:\n",
    "        bow_recom_set[key] = 0\n",
    "        pass\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "for key in bow_recom_set:\n",
    "    bow_recom_set[key] += 1\n",
    "\n",
    "for key in bow_not_recom_set:\n",
    "    bow_not_recom_set[key] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is to make the list dictionary\n",
    "# count how many times each word appears in the reviews\n",
    "bow_recom = collections.Counter([y for x in df_recom.Review for y in x.split()])\n",
    "bow_not_recom = collections.Counter([y for x in df_not_recom.Review for y in x.split()])\n",
    "\n",
    "bow_recom_dict = dict(bow_recom)\n",
    "bow_not_recom_dict = dict(bow_not_recom)\n",
    "\n",
    "for key in bow_recom_dict:\n",
    "    if key not in bow_not_recom_dict:\n",
    "        bow_not_recom_dict[key] = 0\n",
    "        pass\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "for key in bow_not_recom_dict:\n",
    "    if key not in bow_recom_dict:\n",
    "        bow_recom_dict[key] = 0\n",
    "        pass\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "for key in bow_recom_dict:\n",
    "    bow_recom_dict[key] += 1\n",
    "\n",
    "for key in bow_not_recom_dict:\n",
    "    bow_not_recom_dict[key] += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_recom_sum_vals = sum(bow_recom_dict.values())\n",
    "bow_not_recom_sum_vals = sum(bow_not_recom_dict.values())\n",
    "\n",
    "for i in bow_recom_dict:\n",
    "\n",
    "    bow_recom_dict[i] /= bow_recom_sum_vals\n",
    "\n",
    "for i in bow_not_recom_dict:\n",
    "\n",
    "    bow_not_recom_dict[i] /= bow_not_recom_sum_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(bow_recom_dict) == len(bow_not_recom_dict)\n",
    "assert len(bow_recom_set) == len(bow_not_recom_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.drop(columns=[\"set_column\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_sentiment(element, test_all=False, test_only_tf_abs=False, test_only_idf_abs=False, test_only_prob=False, test_all_abs=False):\n",
    "\n",
    "    \"\"\"This function takes a review and returns the label for that review\"\"\"\n",
    "\n",
    "    full_shape = df.shape[0]\n",
    "    positive_review_probabiliy = len(df_recom)/full_shape\n",
    "    negative_review_probabiliy = len(df_not_recom)/full_shape\n",
    "    \n",
    "    df_choices_positive = [positive_review_probabiliy, bow_recom_set, bow_recom_dict, df_recom]\n",
    "    df_choices_negative = [negative_review_probabiliy ,bow_not_recom_set, bow_not_recom_dict, df_not_recom]\n",
    "\n",
    "    def classifier(element, your_class = 'positive'):\n",
    "        flag = {}\n",
    "        flag_count = 0\n",
    "        if your_class == 'positive':\n",
    "            df_choices = df_choices_positive\n",
    "        else:\n",
    "            df_choices = df_choices_negative\n",
    "        prob_of_class = df_choices[0]/full_shape\n",
    "        score = 1 * prob_of_class\n",
    "        score = float(format(score, '.12f'))\n",
    "        # score = 0.1\n",
    "        for i in element.split():\n",
    "            if i not in df_choices[2].keys():\n",
    "                pass\n",
    "            else:\n",
    "                prob_word_given_class = (df_choices[2])[i]\n",
    "                prob_word_given_class = float(format(prob_word_given_class, '.12f'))\n",
    "                # Almost the same value, given our spin on this application. \n",
    "                # Normally, this term frequency would be calculated differently across the positive and negative documents\n",
    "                # but we are only looking at the reviews as the documents themselves to determine a word's relevance in the positive\n",
    "                # or negative corpus. \n",
    "                tf = np.log(prob_word_given_class)\n",
    "                # tf = float(format(tf, '.12f'))\n",
    "                # tf = abs(np.log(prob_word_given_class))\n",
    "                # The IDF is the number of reviews / the number of reviews that contain the word in that given corpus\n",
    "                # idf = abs(np.log(df_choices[3].shape[0]/(df_choices[1])[i]))\n",
    "                idf = np.log(df_choices[3].shape[0]/(df_choices[1])[i])\n",
    "                # idf = float(format(idf, '.12f'))\n",
    "                if test_all:\n",
    "                    score *= prob_word_given_class*tf*idf\n",
    "                elif test_only_tf_abs:\n",
    "                    score *= prob_word_given_class*tf\n",
    "                    score = abs(score)\n",
    "                elif test_only_idf_abs:\n",
    "                    score *= prob_word_given_class*idf\n",
    "                    score = abs(score)\n",
    "                elif test_only_prob:\n",
    "                    score *= prob_word_given_class\n",
    "                elif test_all_abs:\n",
    "                    score *= prob_word_given_class*tf*idf\n",
    "                    score = abs(score)\n",
    "\n",
    "                if score < 0:\n",
    "                    flag[flag_count] = (score, your_class)\n",
    "                    # print(flag_count,i)\n",
    "                    flag_count += 1\n",
    "                    # return 'Flag'\n",
    "\n",
    "\n",
    "        return score\n",
    "    positive_score = classifier(element, 'positive')\n",
    "    negative_score = classifier(element, 'negative')\n",
    "    if positive_score > negative_score:\n",
    "        return True\n",
    "    elif positive_score == negative_score:\n",
    "        # Choosing an arbitrary value, because we assume that a review with one or few words of little substance\n",
    "        # is implied to be negative, as is usual with netizens. \n",
    "        return False\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[\"score_all\"] = test.Review.apply(define_sentiment, test_all=True)\n",
    "test[\"score_tf\"] = test.Review.apply(define_sentiment, test_only_tf_abs=True)\n",
    "test[\"score_idf\"] = test.Review.apply(define_sentiment, test_only_idf_abs=True)\n",
    "test[\"score_freq\"] = test.Review.apply(define_sentiment, test_only_prob=True)\n",
    "test[\"score_mod_all\"] = test.Review.apply(define_sentiment, test_all_abs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the result without the word 'cyberpunk' included in the corpus:\n",
      "This is our accuracy for (unbalanced) real data with all the formula: 48.62%\n",
      "This is our accuracy for (unbalanced) real data with only class probability, frequency, and absolute value of TF: 77.03%\n",
      "This is our accuracy for (unbalanced) real data with only class probability, frequency, and absolute value of IDF: 69.86%\n",
      "This is our accuracy for (unbalanced) real data with only class probability and frequency: 75.04%\n",
      "This is our accuracy for (unbalanced) real data with all the formula, but with the absolute value of the score: 69.85%\n"
     ]
    }
   ],
   "source": [
    "print(\"This is the result without the word 'cyberpunk' included in the corpus:\")\n",
    "print(f\"This is our accuracy for (unbalanced) real data with all the formula: {(sum(test['Recommended or Not Recommended'] == test['score_all'])/test.shape[0])*100:.2f}%\")\n",
    "print(f\"This is our accuracy for (unbalanced) real data with only class probability, frequency, and absolute value of TF: {(sum(test['Recommended or Not Recommended'] == test['score_tf'])/test.shape[0])*100:.2f}%\")\n",
    "print(f\"This is our accuracy for (unbalanced) real data with only class probability, frequency, and absolute value of IDF: {(sum(test['Recommended or Not Recommended'] == test['score_idf'])/test.shape[0])*100:.2f}%\")\n",
    "print(f\"This is our accuracy for (unbalanced) real data with only class probability and frequency: {(sum(test['Recommended or Not Recommended'] == test['score_freq'])/test.shape[0])*100:.2f}%\")\n",
    "print(f\"This is our accuracy for (unbalanced) real data with all the formula, but with the absolute value of the score: {(sum(test['Recommended or Not Recommended'] == test['score_mod_all'])/test.shape[0])*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.34      0.58      0.43      1796\n",
      "        True       0.68      0.44      0.53      3636\n",
      "\n",
      "    accuracy                           0.49      5432\n",
      "   macro avg       0.51      0.51      0.48      5432\n",
      "weighted avg       0.57      0.49      0.50      5432\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate a confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(test['Recommended or Not Recommended'], test['score_all'])\n",
    "# Generate a classification report\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(test['Recommended or Not Recommended'], test['score_all']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.64      0.69      0.67      1796\n",
      "        True       0.84      0.81      0.82      3636\n",
      "\n",
      "    accuracy                           0.77      5432\n",
      "   macro avg       0.74      0.75      0.75      5432\n",
      "weighted avg       0.78      0.77      0.77      5432\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate a confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(test['Recommended or Not Recommended'], test['score_tf'])\n",
    "# Generate a classification report\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(test['Recommended or Not Recommended'], test['score_tf']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.56      0.42      0.48      1796\n",
      "        True       0.75      0.84      0.79      3636\n",
      "\n",
      "    accuracy                           0.70      5432\n",
      "   macro avg       0.65      0.63      0.63      5432\n",
      "weighted avg       0.68      0.70      0.69      5432\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate a confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(test['Recommended or Not Recommended'], test['score_idf'])\n",
    "# Generate a classification report\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(test['Recommended or Not Recommended'], test['score_idf']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.60      0.71      0.65      1796\n",
      "        True       0.84      0.77      0.81      3636\n",
      "\n",
      "    accuracy                           0.75      5432\n",
      "   macro avg       0.72      0.74      0.73      5432\n",
      "weighted avg       0.76      0.75      0.75      5432\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate a confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(test['Recommended or Not Recommended'], test['score_freq'])\n",
    "# Generate a classification report\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(test['Recommended or Not Recommended'], test['score_freq']))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3ad933181bd8a04b432d3370b9dc3b0662ad032c4dfaa4e4f1596c548f763858"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
